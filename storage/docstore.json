{"docstore/metadata": {"1d99d66a-a2c2-42ca-af4d-081762592ca5": {"doc_hash": "f1eb231897a71eecae4f536d9d3f656de0ac6a65110d988425a8540b128e269b"}, "24bad57e-9733-4746-93fc-43dda2c3651c": {"doc_hash": "d68f15dd79ec6bcd67bf7f770bd694c761fb91a654f1927182a4e2390af662c5"}, "2e1fb198-b9e2-421f-94dd-0ec449367447": {"doc_hash": "5a863c029fd7d6811dc63db8a38473df91740bc3b2f6ef020d0a0142ff37fd79"}, "5563b9e3-1b4e-4016-b898-e2ae768e8ffc": {"doc_hash": "a11e62c560fc647555d210f732b5843d8351cec88b59a4bb83ddcbe766e8e621"}, "cb94d43e-267f-4d9d-9fff-fde45c82e081": {"doc_hash": "073127706285e094f92c35dac923208ec97efdc8ca0303f1e11b2d154dd6c4d4"}, "e699b846-2996-42b5-9b4c-bd23f86b09a5": {"doc_hash": "b9fe6f0251940aba7f76d951b205520e6a8846eed799bccec15e76c2ae319b05"}, "a1002e05-5a41-4553-bd02-eff4b776a108": {"doc_hash": "4834ddfcca57de8925e7b28932392855ae6d780a6b519c639d3723cc608d88ee"}, "7e4ea3ec-e9c3-48b4-bfc7-5e47c02a4862": {"doc_hash": "bc00b97c02ac2aa2b3cbb3f63bb1ca25fdecf31fa3c6033c3d9667e80d1e2482"}, "2f65b833-e20b-44a0-ab14-0f5e8432248c": {"doc_hash": "bd6e96a6721e9e1ddf9515a1df3f2fc954aaaffc952214cae3293f65bc3dd6af"}, "a64b368d-09b5-400a-b789-3da3d5174589": {"doc_hash": "89ddfee4c6a3fae7d3d1b569dddbf34ed8c49b614319e5c05b2d4d8fce443460"}, "aa8c4472-898a-4470-8577-2738ad4dabc4": {"doc_hash": "bfc1160bddcc4c183c333ba91ecd0f95e3b04a9df37b913b9765091feacce566"}, "c2eefe78-a0bc-4384-8cfd-0966a628d74c": {"doc_hash": "0c9a2c2f319b4cd300dcef94467383eabc14a8f4a2ca7808f4650ab2dff40257"}, "8f4ba1b4-842a-4a08-951f-8116b6d96196": {"doc_hash": "006031673559ae157e058a323e399212d54dc18cb9dd681978535e8cad28ff79"}, "d27039c7-78a5-4e9f-a358-dfffd25441c8": {"doc_hash": "1eb98eb3213ce0861307277ea3c124481696d19659fac2c7c9b09a2f600fdd5a"}, "a610bb39-38b6-4977-b388-266a83ac9301": {"doc_hash": "115c87c32c31d43c026e3ceae01ca29a6f2c8926e391c54aa1839c409de2c334", "ref_doc_id": "1d99d66a-a2c2-42ca-af4d-081762592ca5"}, "99f341ba-5b9e-4acb-90a1-94ea8365ae7e": {"doc_hash": "c08285cb94efbf8bce6acb4f08257983403dfea41eefecac6b48c5fe14885baa", "ref_doc_id": "1d99d66a-a2c2-42ca-af4d-081762592ca5"}, "8a160006-3b02-45bf-8bf3-e4ca5ad20152": {"doc_hash": "d9c29f27ad52fe08aab62063e37298790da12f1921a4b5783a8529d127ef0511", "ref_doc_id": "24bad57e-9733-4746-93fc-43dda2c3651c"}, "22dbc33e-29bd-4b6b-8441-f1f3bc4f59a0": {"doc_hash": "999463cac915e2b4c9e335dc7b5fa8af26123fac962d5763deb46f4fed7174a6", "ref_doc_id": "24bad57e-9733-4746-93fc-43dda2c3651c"}, "41023438-3c98-4ede-9bfc-2fe8ca5bea76": {"doc_hash": "5a863c029fd7d6811dc63db8a38473df91740bc3b2f6ef020d0a0142ff37fd79", "ref_doc_id": "2e1fb198-b9e2-421f-94dd-0ec449367447"}, "e8f5a8bb-2464-46a0-b4eb-01eb6cf52389": {"doc_hash": "01d160c825c4be4114bf057592ca8cdf6b8a149e24b5f5c2124b91dddf3459e1", "ref_doc_id": "5563b9e3-1b4e-4016-b898-e2ae768e8ffc"}, "f07bbb01-f6b9-44e8-ace8-ade73b2273b6": {"doc_hash": "7cd9542974de65bac83f61b658533a592967c448b3dd7c9e48fb066db1d9fa41", "ref_doc_id": "5563b9e3-1b4e-4016-b898-e2ae768e8ffc"}, "039607ce-6ad3-4464-b98c-86ae000c26ed": {"doc_hash": "073127706285e094f92c35dac923208ec97efdc8ca0303f1e11b2d154dd6c4d4", "ref_doc_id": "cb94d43e-267f-4d9d-9fff-fde45c82e081"}, "d95d3dca-57b1-4063-a278-1b95e01c0632": {"doc_hash": "b6f79d3c63e1053c9a27c4f5604030162971de72613644563b6c3aff56ccb4c5", "ref_doc_id": "e699b846-2996-42b5-9b4c-bd23f86b09a5"}, "73ce5d80-55df-4d82-8396-8ada1c70f9a1": {"doc_hash": "092a70dd5084b872938ca8de7d708cd691a1b866fa494bfd82b4887951d2b43b", "ref_doc_id": "e699b846-2996-42b5-9b4c-bd23f86b09a5"}, "cf26f1c5-5241-4cf8-bdf4-2d1421f04e25": {"doc_hash": "4834ddfcca57de8925e7b28932392855ae6d780a6b519c639d3723cc608d88ee", "ref_doc_id": "a1002e05-5a41-4553-bd02-eff4b776a108"}, "cad9dfd7-db87-4e14-bb98-efeb9db4fdf8": {"doc_hash": "bc00b97c02ac2aa2b3cbb3f63bb1ca25fdecf31fa3c6033c3d9667e80d1e2482", "ref_doc_id": "7e4ea3ec-e9c3-48b4-bfc7-5e47c02a4862"}, "29238968-d3fb-4d06-9134-8afa9e4f964d": {"doc_hash": "bd6e96a6721e9e1ddf9515a1df3f2fc954aaaffc952214cae3293f65bc3dd6af", "ref_doc_id": "2f65b833-e20b-44a0-ab14-0f5e8432248c"}, "2c190a6e-d93c-4f8b-8f84-04bf7ee18d64": {"doc_hash": "b143b2a36e170e6d132d4612f6c32cba757ab9380324760952d173c3589232a8", "ref_doc_id": "a64b368d-09b5-400a-b789-3da3d5174589"}, "403c269e-68c6-4600-b043-9e603d85a890": {"doc_hash": "967a6a5d95311dc0ef426ee85f3f5559a0422849acafdc2e4f9acb9f0318f48f", "ref_doc_id": "a64b368d-09b5-400a-b789-3da3d5174589"}, "97d0512d-9ce9-452f-a0ca-4b9b81c81602": {"doc_hash": "8fb880e47bb76132701d074fe2614366ae62e5e280bbe58ed80adbe4d855aa82", "ref_doc_id": "aa8c4472-898a-4470-8577-2738ad4dabc4"}, "b07ad015-a3bc-4ed6-b3e8-2321877cfc74": {"doc_hash": "19380cc2445dc635a044b29275cbeeff09b498f977112394364a6484af1ea127", "ref_doc_id": "aa8c4472-898a-4470-8577-2738ad4dabc4"}, "66920057-c461-4dd5-aeea-482ac83ba272": {"doc_hash": "73cd26f2ca3d33ad3c0c1acf3830756516057ae07ef8a0d6d1c0deb2c65d253a", "ref_doc_id": "c2eefe78-a0bc-4384-8cfd-0966a628d74c"}, "0ed0e6d9-d453-4835-ad1f-672a95ce0685": {"doc_hash": "dac9d9934739764d6a8fcc520b85d969d57d7331e763b9e04980a875881cc756", "ref_doc_id": "c2eefe78-a0bc-4384-8cfd-0966a628d74c"}, "034c1dce-5fc9-4c86-81bc-e204afa3fbce": {"doc_hash": "006031673559ae157e058a323e399212d54dc18cb9dd681978535e8cad28ff79", "ref_doc_id": "8f4ba1b4-842a-4a08-951f-8116b6d96196"}, "bd9731a7-724c-4f69-94f0-abe5d250c377": {"doc_hash": "1eb98eb3213ce0861307277ea3c124481696d19659fac2c7c9b09a2f600fdd5a", "ref_doc_id": "d27039c7-78a5-4e9f-a358-dfffd25441c8"}}, "docstore/data": {"a610bb39-38b6-4977-b388-266a83ac9301": {"__data__": {"id_": "a610bb39-38b6-4977-b388-266a83ac9301", "embedding": null, "metadata": {"page_label": "1", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1d99d66a-a2c2-42ca-af4d-081762592ca5", "node_type": "4", "metadata": {"page_label": "1", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "hash": "f1eb231897a71eecae4f536d9d3f656de0ac6a65110d988425a8540b128e269b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99f341ba-5b9e-4acb-90a1-94ea8365ae7e", "node_type": "1", "metadata": {}, "hash": "dee13d02fd9ea2fbf83a4b9e45613a7cc36dafba9fa29971c0d349bbaa14f2f5", "class_name": "RelatedNodeInfo"}}, "text": "Arxiv Preprint.\nFACT, FETCH ,AND REASON : A U NIFIED EVALUATION\nOFRETRIEVAL -AUGMENTED GENERATION\nSatyapriya Krishna\u2217\nHarvard University\nKalpesh Krishna\u2020\u2020, Anhad Mohananey\u2020, Steven Schwarcz, Adam Stambler\nShyam Upadhyay &Manaal Faruqui\nGoogle, Inc.\nABSTRACT\nLarge Language Models (LLMs) have demonstrated significant performance\nimprovements across various cognitive tasks. An emerging application is using\nLLMs to enhance retrieval-augmented generation (RAG) capabilities. These\nsystems require LLMs to understand user queries, retrieve relevant information,\nand synthesize coherent and accurate responses. Given the increasing real-world\ndeployment of such systems, comprehensive evaluation becomes crucial. To this\nend, we propose FRAMES (Factuality, Retrieval, And reasoning MEasurement Set),\na high-quality evaluation dataset designed to test LLMs\u2019 ability to provide factual\nresponses, assess retrieval capabilities, and evaluate the reasoning required to\ngenerate final answers. While previous work has provided datasets and benchmarks\nto evaluate these abilities in isolation, FRAMES offers a unified framework that\nprovides a clearer picture of LLM performance in end-to-end RAG scenarios. Our\ndataset comprises challenging multi-hop questions that require the integration of\ninformation from multiple sources. We present baseline results demonstrating that\neven state-of-the-art LLMs struggle with this task, achieving 0.40 accuracy with\nno retrieval. The accuracy is significantly improved with our proposed multi-step\nretrieval pipeline, achieving an accuracy of 0.66 (>50% improvement). We hope\nour work will help bridge evaluation gaps and assist in developing more robust and\ncapable RAG systems.\n1 I NTRODUCTION\nRecent advancements in Large Language Models (LLMs) have significantly enhanced their\ncapabilities across various natural language processing tasks, especially in systems that demand\nboth factual accuracy and sophisticated reasoning for complex queries (Zhao et al., 2023). Retrieval-\naugmented generation (RAG) techniques (Lewis et al., 2020; Fan et al., 2019; Guu et al., 2020) have\nbecome a powerful approach by leveraging the strengths of retrieval systems and the generative\ncapabilities of LLMs. These techniques are particularly effective for tasks requiring multi-hop\nreasoning, factual grounding, and synthesizing information from diverse knowledge domains (Gao\net al., 2023). However, despite this progress, the evaluation of RAG systems remains fragmented and\ninsufficient, as existing benchmarks typically assess components like retrieval, factual correctness,\nand reasoning in isolation (Yu et al., 2024a). This piecemeal approach fails to capture the holistic\nperformance of these systems in real-world applications (Yu et al., 2024b).\nTo bridge this gap, we introduce a novel evaluation framework, FRAMES1(Factuality, Retrieval, And\nreasoning MEasurement Set), designed to rigorously test LLMs on all three core capabilities\u2014fact\nretrieval, reasoning across multiple constraints, and accurate synthesis of information into coherent\nresponses. Unlike existing datasets such as TruthfulQA (Lin et al., 2021), HotpotQA (Yang et al.,\n2018b), or GSM8k (Cobbe et al., 2021), which focus on isolated aspects of LLM performance,\n\u2217Work done as an intern at Google.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3298, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "99f341ba-5b9e-4acb-90a1-94ea8365ae7e": {"__data__": {"id_": "99f341ba-5b9e-4acb-90a1-94ea8365ae7e", "embedding": null, "metadata": {"page_label": "1", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1d99d66a-a2c2-42ca-af4d-081762592ca5", "node_type": "4", "metadata": {"page_label": "1", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "hash": "f1eb231897a71eecae4f536d9d3f656de0ac6a65110d988425a8540b128e269b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a610bb39-38b6-4977-b388-266a83ac9301", "node_type": "1", "metadata": {"page_label": "1", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "hash": "115c87c32c31d43c026e3ceae01ca29a6f2c8926e391c54aa1839c409de2c334", "class_name": "RelatedNodeInfo"}}, "text": "Corresponding Author: skrishna@g.harvard.edu\n\u2020Equal contribution\n1Dataset link : https://huggingface.co/datasets/google/frames-benchmark\n1arXiv:2409.12941v1  [cs.CL]  19 Sep 2024", "mimetype": "text/plain", "start_char_idx": 3299, "end_char_idx": 3477, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a160006-3b02-45bf-8bf3-e4ca5ad20152": {"__data__": {"id_": "8a160006-3b02-45bf-8bf3-e4ca5ad20152", "embedding": null, "metadata": {"page_label": "2", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "24bad57e-9733-4746-93fc-43dda2c3651c", "node_type": "4", "metadata": {"page_label": "2", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "hash": "d68f15dd79ec6bcd67bf7f770bd694c761fb91a654f1927182a4e2390af662c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22dbc33e-29bd-4b6b-8441-f1f3bc4f59a0", "node_type": "1", "metadata": {}, "hash": "0ccf65d58c7afc1d1e83eed1ac986e11de64034e8cbd1b8b119d46cfdac7d934", "class_name": "RelatedNodeInfo"}}, "text": "Arxiv Preprint.\nTable 1: Comparison of FRAMES against other datasets. FRAMES provides a combination of evaluation samples\nto test the factuality, retrieval, and reasoning of RAG systems. The dataset also covers multi-hop/step questions\nalong with temporal disambiguation.\nDataset Factuality Retrieval ReasoningMulti-\nHop/StepTemporal\nDisambiguation\nFRAMES \u2714 \u2714 \u2714 \u2714 \u2714\nTruthfulQA (Lin et al., 2021) \u2714 \u2717 \u2717 \u2717 \u2717\nOpenbookQA (Mihaylov et al., 2018) \u2714 \u2717 \u2717 \u2717 \u2717\nHotpotQA (Yang et al., 2018b) \u2714 \u2717 \u2717 \u2714 \u2717\nHybridQA (Chen et al., 2020) \u2717 \u2717 \u2714 \u2714 \u2714\nGSM8k (Cobbe et al., 2021) \u2717 \u2717 \u2714 \u2714 \u2717\nMultihop-RAG(Tang & Yang, 2024) \u2714 \u2714 \u2717 \u2714 \u2717\nMoreHopQA (Schnitzler et al., 2024) \u2714 \u2717 \u2714 \u2714 \u2717\nMuSiQue (Trivedi et al., 2022) \u2714 \u2717 \u2714 \u2714 \u2717\nNaturalQuestions (Kwiatkowski et al., 2019) \u2717 \u2714 \u2717 \u2717 \u2714\nTriviaQA (Joshi et al., 2017) \u2714 \u2717 \u2717 \u2717 \u2717\nELI5 (Fan et al., 2019) \u2717 \u2714 \u2714 \u2717 \u2717\nFRAMES provides an integrated evaluation that challenges models across these dimensions\nsimultaneously. This approach offers a more accurate reflection of how these systems perform\nas end-to-end reasoning solutions, especially in scenarios requiring multi-document retrieval and\ncomplex reasoning. For instance, a sample from our dataset is \"How many years earlier would\nPunxsutawney Phil have to be canonically alive to have made a Groundhog Day prediction in the\nsame state as the US capitol?\" This requires the model to perform temporal and numerical reasoning\nafter retrieving the critical articles needed to answer the question.\nOur work addresses a critical void in the current landscape by offering a challenging evaluation\nbenchmark that not only tests the individual components of LLMs but also evaluates their performance\nin an end-to-end context. Through our dataset, we simulate realistic, multi-document queries to\nassess the ability of LLMs to retrieve relevant facts, reason accurately, and synthesize information\ninto coherent responses. Additionally, we present empirical results on the performance of state-of-\nthe-art models, highlighting both their strengths and the limitations in their reasoning capabilities.\nThese findings pave the way for further research and development of more robust and efficient\nretrieval-augmented generation systems. Our key contributions are as follows:\n\u2022We introduce FRAMES , a novel dataset of 824 test samples designed to evaluate LLMs\u2019\nability to retrieve and reason across multiple documents in a unified framework.\n\u2022We provide a comprehensive evaluation of state-of-the-art LLMs, highlighting their\nperformance on factuality, retrieval, and reasoning tasks across diverse domains.\n\u2022We present new empirical insights into the limitations of existing LLMs in handling multi-\nhop and temporal reasoning tasks, offering avenues for future research to improve these\nsystems.\n\u2022We propose a multi-step retrieval and reasoning framework that compels models to iteratively\nretrieve and reason, significantly enhancing their performance on complex queries.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2926, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "22dbc33e-29bd-4b6b-8441-f1f3bc4f59a0": {"__data__": {"id_": "22dbc33e-29bd-4b6b-8441-f1f3bc4f59a0", "embedding": null, "metadata": {"page_label": "2", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "24bad57e-9733-4746-93fc-43dda2c3651c", "node_type": "4", "metadata": {"page_label": "2", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "hash": "d68f15dd79ec6bcd67bf7f770bd694c761fb91a654f1927182a4e2390af662c5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a160006-3b02-45bf-8bf3-e4ca5ad20152", "node_type": "1", "metadata": {"page_label": "2", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "hash": "d9c29f27ad52fe08aab62063e37298790da12f1921a4b5783a8529d127ef0511", "class_name": "RelatedNodeInfo"}}, "text": "2FRAMES\nFRAMES (Factuality, Retrieval, And reasoning MEasurement Set) is an evaluation set of 824 questions\ndesigned to provide an end-to-end evaluation of Retrieval Augmented Generation (RAG) systems.\nIt assesses three key components of a RAG system: Factuality, Retrieval, and Reasoning. Unlike\nmost existing datasets and benchmarks that evaluate each of these RAG components in isolation,\nFRAMES offers a comprehensive test bed to gain a clear understanding of the overall quality of RAG\nsystems(Lin et al., 2022; Yang et al., 2018a; Welbl et al., 2017). This holistic approach allows for\na more accurate reflection of how these systems perform in real-world scenarios. In this section,\nwe first detail our data collection process, which involved both synthetic data generation attempts\nand human annotation. Next, we present the dataset statistics, showcasing the diversity of topics\nand reasoning types covered. Finally, we outline the rigorous quality checks implemented to ensure\n2", "mimetype": "text/plain", "start_char_idx": 2927, "end_char_idx": 3915, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "41023438-3c98-4ede-9bfc-2fe8ca5bea76": {"__data__": {"id_": "41023438-3c98-4ede-9bfc-2fe8ca5bea76", "embedding": null, "metadata": {"page_label": "3", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2e1fb198-b9e2-421f-94dd-0ec449367447", "node_type": "4", "metadata": {"page_label": "3", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "hash": "5a863c029fd7d6811dc63db8a38473df91740bc3b2f6ef020d0a0142ff37fd79", "class_name": "RelatedNodeInfo"}}, "text": "Arxiv Preprint.\nTask Instruction Prompt for Human Annotation\nTask Description\nCreate nfactoid questions that demand multi-hop reasoning based on information found\nacross multiple Wikipedia articles. These questions should ideally have a single, unambiguous\nanswer and may optionally incorporate elements of challenging reasoning.\nHere\u2019s a breakdown of the key terms and their relationships:\n\u2022Factoid Questions: These are trivia-style questions with a single, clearly defined,\nand factually correct answer.\n\u2022Multi-hop Reasoning: This refers to the core requirement that answering the\nquestions necessitates combining information from different sections within multiple\nWikipedia articles. The example provided (\"What is the name of the river that flows\nthrough the city where the Eiffel Tower is located?\") highlights how this differs from\na simple factoid question (\"What is the capital of France?\").\n\u2022Challenging Reasoning: This aspect encourages the inclusion of questions that\ngo beyond simple information retrieval and demand critical thinking. This can be\nachieved through various question types like:\n\u2013Numerical Reasoning: Involving counting, comparisons, or calculations.\n<examples>\n\u2013Tabular Reasoning: Involving statistics found in tables / info boxes in wikipedia.\n<examples>\n\u2013Multiple Constraints: Questions involving multiple constraints, whose\nintersection points towards a unique answer. <examples>\n\u2013Temporal Reasoning : Questions involving reasoning through timelines.\n<examples>\n\u2013Post processing: This involves requiring the answerer to perform specific post-\nprocessing steps after all necessary facts have been gathered. <examples>\nAdditional Requirements:\n\u2022Wikipedia Articles: The information used to answer the questions must be sourced\nfrom Wikipedia articles.\n\u2022Standalone and Context-Independent: Questions should be understandable without\nrequiring additional information or context.\n\u2022Single, Unambiguous Answer: Each question should have only one correct answer,\nleaving no room for ambiguity.\n\u2022Avoid boolean questions (yes/no questions) that can be answered with a simple\n\"yes\" or \"no.\" <examples>\nFigure 1: Task instruction provided to human annotators to generate samples for FRAMES .\nthe dataset\u2019s reliability and challenging nature. By providing this end-to-end evaluation framework,\nFRAMES aims to bridge the gap in existing benchmarks and foster the development of more robust\nand efficient RAG systems.\nSynthetic Data Generation Attempts. We start our data collection process with synthetic dataset\ngeneration to explore a potentially cost-effective alternative to expensive human annotation. We\nprompt a state-of-the-art LLM with instructions to use multiple articles to generate questions that\nwould require information from multiple articles to answer. The prompt (shown in Figure 6 in\nAppendix A) takes as input the number of articles provided to generate questions. However, we\nobserved significant issues with this approach. While the LLMs were able to generate coherent\nquestions, there was a high proportion of hallucinated questions and answers (>30%). Additionally,\nthe LLM struggled to generate questions that strictly required more than four articles. To evaluate\nthe potential of this approach, we manually cleaned the hallucinated questions and answers from the\nobtained set. We then evaluated the same LLM on these cleaned questions and obtained an accuracy\n3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3405, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8f5a8bb-2464-46a0-b4eb-01eb6cf52389": {"__data__": {"id_": "e8f5a8bb-2464-46a0-b4eb-01eb6cf52389", "embedding": null, "metadata": {"page_label": "4", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5563b9e3-1b4e-4016-b898-e2ae768e8ffc", "node_type": "4", "metadata": {"page_label": "4", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "hash": "a11e62c560fc647555d210f732b5843d8351cec88b59a4bb83ddcbe766e8e621", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f07bbb01-f6b9-44e8-ace8-ade73b2273b6", "node_type": "1", "metadata": {}, "hash": "b798065a3efcda0380095b8713d7e4f67928c642a0d4d5f49cc8c6fafa9a938b", "class_name": "RelatedNodeInfo"}}, "text": "Arxiv Preprint.\nTable 2: This table provides descriptions of the different reasoning types to which each question in FRAMES\nbelongs. The distribution of samples belonging to each reasoning type is shown in Figure 2.\nReasoning Type Description\nNumerical Reasoning This involves counting, comparisons, or calculations. For example, the question\n\"How many times faster is the second fastest bird in the Americas compared\nto the fastest human in the world? Round to the nearest integer.\" asks for a\ncalculation comparing the speeds of two objects.\nTabular Reasoning This involves statistics found in tables or infoboxes in Wikipedia. For example,\nthe question \"How many runs did the West Indies vice-captain score in the 1983\nWorld Cup?\" requires the answerer to analyze tabular data of top run scorers and\nextract the relevant information.\nMultiple Constraints This involves questions with multiple constraints whose intersection points\ntowards a unique answer. For example, \"I\u2019m thinking of an airport near the\nintersection of US-52 and I-95. Can you remind me which one it is?\" This query\nhas two constraints: first, to locate an airport, and second, that it should be near\nthe intersection of US-52 and I-95.\nTemporal Reasoning This involves reasoning through timelines. For example, \"Leonardo DiCaprio\nonce won an Oscar for best actor. Who won the award for best costume design\nsixteen years earlier?\" .\nPost-Processing This requires the answerer to perform specific post-processing steps after all\nnecessary facts have been gathered. For example, consider the question: \"What\nis five years after the founding of the largest country in North America in Roman\nnumerals?\" . This question requires the following sub-instructions: (1) Numerical\nreasoning: Add five years to the founding date, and (2) Post-processing: Convert\nthe resulting year into Roman numerals.\nof\u223c32%, suggesting that the legitimate questions generated by LLMs were indeed challenging for\nstate-of-the-art models. There are two key takeaways from our experimentation with synthetic data\ngeneration: (1) Synthetic test data requires heavy manual cleaning before usage, which suggests that\nwe will need to rely on human annotations instead of LLMs to generate the final evaluation set; and\n(2) models performed significantly poorly on the correct test samples we tested on, suggesting that\nthe instruction to create questions can be used to generate a challenging evaluation set.\nHuman annotation. Given these findings, we decided to use the core instruction for generating\nquestions that combine information from multiple articles as a guide for human annotation, shown in\nFigure 1. This approach aimed to leverage the challenging nature of the synthetic questions while\nalso mitigating the issues of hallucination present in LLM-generated content. Human annotators were\ntasked with creating questions that required information from multiple Wikipedia articles, following\na similar structure to the synthetic prompts but with greater reliability and accuracy. The outcome\nof this human annotation resulted in 824 questions with their correct responses along with the list\nof Wikipedia articles needed to answer the questions. We also ask the human annotators to label\neach question based on five reasoning types, i.e, Numerical Reasoning, Tabular Reasoning, Multiple\nConstraints, Temporal Reasoning, and Post-Processing, described in more details in Table 2. Please\nnote that a question can belong to multiple reasoning types. To ensure the highest quality annotations,\nwe engaged a team of carefully vetted experts with extensive experience in question generation and\ncomplex reasoning tasks.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3661, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f07bbb01-f6b9-44e8-ace8-ade73b2273b6": {"__data__": {"id_": "f07bbb01-f6b9-44e8-ace8-ade73b2273b6", "embedding": null, "metadata": {"page_label": "4", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5563b9e3-1b4e-4016-b898-e2ae768e8ffc", "node_type": "4", "metadata": {"page_label": "4", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "hash": "a11e62c560fc647555d210f732b5843d8351cec88b59a4bb83ddcbe766e8e621", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8f5a8bb-2464-46a0-b4eb-01eb6cf52389", "node_type": "1", "metadata": {"page_label": "4", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "hash": "01d160c825c4be4114bf057592ca8cdf6b8a149e24b5f5c2124b91dddf3459e1", "class_name": "RelatedNodeInfo"}}, "text": "Dataset Statistics. The dataset comprises questions related to a diverse set of topics from\nWikipedia, involving subjects such as history, sports, science, animals, health, etc. Each question\nin our dataset require 2-15 Wikipedia articles to answer, with the distribution of the percentage of\ndataset requiring different numbers of Wikipedia articles shown in Figure 2 (left). Approximately\n36% of questions require two articles to answer, \u223c35% require three articles, \u223c16% require four\narticles, and so on. This distribution also represents the general trend of queries asked from LLMs in\nthe real world (Liu et al., 2009), since the proportion of questions requiring two articles is higher\nthan more complicated questions requiring a greater number of articles. Additionally, we have a\nhealthy distribution of questions belonging to different reasoning types, shown in Figure 2 (right).\n4", "mimetype": "text/plain", "start_char_idx": 3662, "end_char_idx": 4552, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "039607ce-6ad3-4464-b98c-86ae000c26ed": {"__data__": {"id_": "039607ce-6ad3-4464-b98c-86ae000c26ed", "embedding": null, "metadata": {"page_label": "5", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb94d43e-267f-4d9d-9fff-fde45c82e081", "node_type": "4", "metadata": {"page_label": "5", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "hash": "073127706285e094f92c35dac923208ec97efdc8ca0303f1e11b2d154dd6c4d4", "class_name": "RelatedNodeInfo"}}, "text": "Arxiv Preprint.\n2 3 4 5 6 7 8 9 10 11\nNumber of articles0102030PercentagePercentage for each number of articles\n0 10 20 30\nPercentageMultiple constraints\nNumerical reasoning\nT emporal reasoning\nT abular reasoning\nPost processingReasoning TypePercentage of Each Reasoning Type\nFigure 2: The figure shows the distribution of questions in the FRAMES , with the percentage of questions\nrequiring different numbers of Wikipedia articles (left) and the percentage of the dataset belonging to each\nreasoning type (right). Please note that the percentage bar for 11 denotes the percentage of questions requiring\n11 or more Wikipedia articles.\nQuestions requiring reasoning over multiple constraints hold the highest percentage of data samples\nin the test ( \u223c36%), followed by questions requiring numerical reasoning ( \u223c20%). Please note that\nmany questions in the dataset also require a combination of different reasoning abilities to find an\nanswer.\nQuality Checks. Other than the data collection process described in the section above, human\nannotators also implemented several quality checks to ensure the dataset\u2019s high quality and\neffectiveness in evaluating RAG capabilities:\n\u2022Ensuring correctness and groundedness to Wikipedia: We verified the correctness of\nquestions and their corresponding answers by re-annotating the questions. Human annotators\nwere asked to confirm if the provided answer was correct and could be answered using\nthe associated Wikipedia pages. This annotation process was conducted three months after\ncollecting the initial data, filtering out 5.5% of samples where the answer was no longer true\nafter that period.\n\u2022Removing ambiguity due to freshness (temporal disambiguation): Annotators added\nextra context to disambiguate answers that could change over time. For example, a question\nlike\"Which country were holders of the FIFA World Cup the last time the UEFA Champions\nLeague was won by a club from London?\" was revised to \"As of August 1, 2024, which\ncountry were holders of the FIFA World Cup the last time the UEFA Champions League was\nwon by a club from London?\" . This approach mitigates issues with frequent manual updates\nrequired for maintaining previous datasets (Vu et al., 2023; Kasai et al., 2024).\n\u2022Preventing guesswork by ensuring a large output space: We removed questions with\nbinary answers (\"yes\" or \"no\") to prevent LLMs from achieving 50% accuracy through\nrandom guessing. This ensures the dataset is challenging enough to clearly evaluate LLM\ncapabilities.\n\u2022Ensuring dataset interpretability and reliability: We limited the articles to those from\nWikipedia, which has a lower chance of containing unreliable information compared to other\nsources.\n\u2022Addressing contamination issues: To mitigate concerns about potential contamination\ndue to Wikipedia articles being in LLM training sets, we designed questions that require\nadditional reasoning and operations beyond simple fact retrieval. For instance, the question\n\"How many years earlier would Punxsutawney Phil have to be canonically alive to have\nmade a Groundhog Day prediction in the same state as the US capitol?\" requires not only\nfact extraction but also additional calculations.\n3 E MPIRICAL ANALYSIS\nAfter obtaining a high-quality test set, we evaluate state-of-the-art LLMs on their ability to answer\nquestions that require proficiency in factuality, retrieval, and reasoning. Our analysis is divided into\n5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3415, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d95d3dca-57b1-4063-a278-1b95e01c0632": {"__data__": {"id_": "d95d3dca-57b1-4063-a278-1b95e01c0632", "embedding": null, "metadata": {"page_label": "6", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e699b846-2996-42b5-9b4c-bd23f86b09a5", "node_type": "4", "metadata": {"page_label": "6", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "hash": "b9fe6f0251940aba7f76d951b205520e6a8846eed799bccec15e76c2ae319b05", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "73ce5d80-55df-4d82-8396-8ada1c70f9a1", "node_type": "1", "metadata": {}, "hash": "bcae54dcbbc77ec5711fe24ab9cfb9f9d0906ff61daf61e96a791557b95f3b37", "class_name": "RelatedNodeInfo"}}, "text": "Arxiv Preprint.\ntwo sections: (1) Single-step Evaluations (Section 3.1): Here, we evaluate the LLMs based on a\nsingle-shot inference, where the idea is to ask the question and assess the response after a single\ninference call. This evaluation is further divided into cases with and without retrieval to analyze the\nimpact of retrieval on performance. (2) Multi-Step Evaluations (Section 3.2): In this case, we evaluate\nthe models after making more than a single inference step, focusing on scenarios where retrieval is\nexplicitly required. The motivation for multi-step evaluations is to determine whether forcing the\nmodel to retrieve and reason across multiple steps could lead to performance improvements. Next,\nwe describe the details of the two sets of experiments.\n3.1 S INGLE -STEPEVALUATIONS\nIn this set of experiments, we evaluate the model using several baseline prompting methods on our test\nset to understand how well existing LLMs perform. Specifically, we experiment with three baseline\napproaches: (1) Naive Prompt: This is a straightforward approach where we simply ask the question\nto the model and evaluate if the model\u2019s response without search retrieval contains the correct answer.\n(2) BM25-Retrieved Prompt (n_docs): This approach augments the question with the top n_docs\ndocuments having the highest BM25 score (Robertson et al., 1995) retrieved from a Wikipedia data\ndump2. The BM25 score is computed between the question and every article in the Wikipedia dump,\nafter which the top n_docs with the highest scores are added to the prompt. The motivation behind\nthis approach is to observe improvements in model performance when relevant articles are added to\nthe context. This is denoted as BM25-R (n_doc) in the results. (3) Oracle Prompt: This prompt\nincludes the question along with all the ground truth Wikipedia articles used by the human annotators\nto generate the question. The performance of the Oracle Prompt provides the upper bound of model\nperformance in the case of a perfect retrieval system that is able to extract all the relevant articles.\nExperiment Setup. For the experiments, we use Gemini-Pro-1.5-0514 (Google, 2024b),\nGemini-Flash-1.5-0514 (Google, 2024a), Gemma2-27b (Gemma et al., 2024), and\nGemma2-9b (Gemma et al., 2024) as the state-of-the-art LLM since they have shown great\nperformance on several public benchmarks. Since the gold answers to questions in the dataset\nare free-form tokens instead of choices from multiple-choice answers, we use an LLM to evaluate\nif the outcome from the LLM under evaluation matches the gold answer, using the prompt shown\nin Figure 7 in Appendix B. This auto-rating mechanism was tested against human evaluations, in\nwhich the LLM-based evaluation showed strong alignment with human annotations (accuracy: 0.96\nand Cohen\u2019s Kappa: 0.889 for Gemini-Pro-1.5-0514 as autorating LLM), making LLM-based\nevaluation a suitable approach to evaluate the correctness of model responses.\nLLMs perform poorly in single-step evaluations. Based on results shown in Table 3, we observe\nthat naive prompting attains a performance of \u223c40% with gradual increases when including BM25\nretrieved articles for Gemini-Pro-1.5-0514 .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3196, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "73ce5d80-55df-4d82-8396-8ada1c70f9a1": {"__data__": {"id_": "73ce5d80-55df-4d82-8396-8ada1c70f9a1", "embedding": null, "metadata": {"page_label": "6", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e699b846-2996-42b5-9b4c-bd23f86b09a5", "node_type": "4", "metadata": {"page_label": "6", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "hash": "b9fe6f0251940aba7f76d951b205520e6a8846eed799bccec15e76c2ae319b05", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d95d3dca-57b1-4063-a278-1b95e01c0632", "node_type": "1", "metadata": {"page_label": "6", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "hash": "b6f79d3c63e1053c9a27c4f5604030162971de72613644563b6c3aff56ccb4c5", "class_name": "RelatedNodeInfo"}}, "text": "The model achieves an accuracy of \u223c45% when\nthe number of documents in the context is 2, and \u223c47% when double the number of articles are\nadded to the context. These improvements demonstrate the room for enhancement when the model\nis able to retrieve relevant articles required to answer the question. The core reason behind these\nimprovements is the improvement in recall in the articles present in context which increased from\n0.12 (BM25-R(n_docs = 2) to 0.15 (BM25-R (n_docs = 4)). In addition to these approaches, we\nobserve an accuracy of \u223c72% for Gemini-Pro-1.5-0514 when all the gold Wikipedia articles\nare provided in the context, which we call Oracle Prompt. Out of \u223c28% samples where the model\nmade errors, \u223c80% of those misclassifications belong to numerical, tabular, and post-processing\ncategories. Hence, these misclassifications show the reasoning gaps in model performance where\neven after providing all the relevant facts, the model failed to reason through the different facts to\nprovide a correct answer to the question. The accuracies obtained by the Naive Prompt and Oracle\nPrompt can be considered as the lower bound (when no relevant articles were provided to the model)\nand upper bound (when all relevant articles were provided to the model) of model performances on\nFRAMES . This pattern can also be seen in Figure 3 where we plotted accuracy for each reasoning\ntype and observe that the model performed the lowest in numerical, post-processing, and tabular\nreasoning tasks. We also observe that adding BM25 retrieved articles primarily helped with questions\nrequiring reasoning through multiple constraints ( \u223c8% improvement) and post-processing ( \u223c10%\n2https://www.tensorflow.org/datasets/catalog/wikipedia#\nwikipedia20230601en_default_config\n6", "mimetype": "text/plain", "start_char_idx": 3197, "end_char_idx": 4967, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf26f1c5-5241-4cf8-bdf4-2d1421f04e25": {"__data__": {"id_": "cf26f1c5-5241-4cf8-bdf4-2d1421f04e25", "embedding": null, "metadata": {"page_label": "7", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a1002e05-5a41-4553-bd02-eff4b776a108", "node_type": "4", "metadata": {"page_label": "7", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "hash": "4834ddfcca57de8925e7b28932392855ae6d780a6b519c639d3723cc608d88ee", "class_name": "RelatedNodeInfo"}}, "text": "Arxiv Preprint.\nMultiple constraintsNumerical reasoningPost processingT abular reasoningT emporal reasoning\nReasoning Type0.00.10.20.30.40.50.60.7AccuracyAccuracy of Each Reasoning Type\nBaselines\nNaive Prompting\nBM25-Retrieved Prompt (n_doc=2)\nBM25-Retrieved Prompt (n_doc=4)\nOracle\nFigure 3: Accuracy of Gemini-Pro-1.5-0514 across different reasoning types in our test set. The results\nindicate superior performance on logical and temporal reasoning tasks, with notable deficiencies in numerical,\ntabular, and post-processing reasoning. The substantial performance improvements observed with oracle\ninformation underscore the critical role of relevant contextual information in enhancing model accuracy across\nall reasoning categories.\nTable 3: This table presents the accuracy performance of Gemini-Pro-1.5-0514 (G-Pro-1.5),\nGemini-Flash-1.5-0514 (G-Flash-1.5), Gemma2-27b , and Gemma2-9b on our proposed evaluation\ndataset. Please note that the performance of Gemma models is not reported for cases requiring longer context\ndue to the small maximum context length of the model.\nBaselines G-Pro-1.5 G-Flash-1.5 Gemma2-27b Gemma2-9b\nNaive Prompt 0.408 0.263 0.308 0.051\nBM25-R (n_doc=2) 0.452 0.288 - -\nBM25-R (n_doc=4) 0.474 0.315 - -\nOracle Prompt 0.729 0.665 - -\nimprovement). This aligns well with the fact that providing more relevant articles helps in obtaining\nfacts for each constraint, leading to improvements in performance. We take these learnings and\nexperiment with a more complicated setup where the model is asked to find answer to questions\nthrough multiple iterations instead of a single step.\n3.2 M ULTI -STEPEVALUATIONS\nBased on the findings from the previous experiment with single-step evaluations, where we observed\nan increase in performance when related articles are added to the context, wew were led to explore a\nsetting where the model is compelled to plan its search for relevant Wikipedia articles in order to\nfind answers. More specifically, we design a pipeline where the model is asked a question along\nwith the instruction to generate ksearch queries which are then used to extract the top-n_docs\nWikipedia articles with the highest BM25 scores. These documents are then appended to the context.\nThis process of query generation and retrieved article augmentation is carried forward for nsteps.\nOnce the nsteps of retrieval are completed, the model is asked to answer the question based on\nthe articles appended in the context, as shown in Algorithm 1. We conduct two sets of experiments\nhere: (1) Vanilla with no explicit planning instructions, and (2) With search planning instructions\nto help the model navigate the search process efficiently. To implement this pipeline, we used the\nsimplest document retrieval component which is essentially an index of Wikipedia pages, where\nthe articles with the highest BM25 scores for each query are returned to the LLM and added to the\n7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2914, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cad9dfd7-db87-4e14-bb98-efeb9db4fdf8": {"__data__": {"id_": "cad9dfd7-db87-4e14-bb98-efeb9db4fdf8", "embedding": null, "metadata": {"page_label": "8", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7e4ea3ec-e9c3-48b4-bfc7-5e47c02a4862", "node_type": "4", "metadata": {"page_label": "8", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "hash": "bc00b97c02ac2aa2b3cbb3f63bb1ca25fdecf31fa3c6033c3d9667e80d1e2482", "class_name": "RelatedNodeInfo"}}, "text": "Arxiv Preprint.\nAlgorithm 1 Multi-Step Evaluation with BM25 Retrieval\n1:Input: Initial question Q, number of iterations n, number of queries k, number of documents\nn_docs\n2:Output: Final response R\n3:Initialize context C\u2190 {Q}\n4:foriteration i= 1tondo\n5: forquery j= 1tokdo\n6: Generate search query Qijbased on context C\n7: Retrieve top n_docs documents Dijusing BM25 based on Qij\n8: C\u2190C\u222a(Dij\\C){Add only new documents to context}\n9: end for\n10:end for\n11:Generate final response Rusing context C\n12:return R\nFigure 4: The figure shows the performance improvements when the number of steps (n) and number of queries\nper step (k) is changed. We achieved the best performance of 0.66 with the combination of ( k,n,n_docs ) =\n(5,5,10)\ncontext. This retrieval component is used instead of making direct calls to an online search engine for\ntwo reasons: (1) We would like to keep the retrieval system constant to clearly evaluate the search\nplanning capability of the LLMs instead of the retrieval system\u2019s capability in returning the most\nrelevant articles, and (2) The BM25-based retrieval system makes our pipeline reproducible and\nlimits the search space to Wikipedia pages only, as the questions were generated from Wikipedia\narticles only.\nMulti-Step retrievals significantly improve model performance. We plot model performance\non different combinations of ( k,n,n_docs ) in Figure 5. Based on these results, we observe a steady\nincrease in performance as the number of steps and queries are increased with accuracy increasing\nfrom\u223c0.45to\u223c0.52for the case of ( k,n,n_docs )=(5,5,2) for vanilla setting where the model is\nnot provided with any specific planning instructions. This is expected, as more steps and queries\nallow the model to add more relevant documents to its context, leading the model to improve recall,\nwhich translates to better accuracy. However, the performance still remains quite low even after\nfive iterations of search retrievals, which is computationally expensive since this requires six non-\nparallelizable inference calls (five for retrieval + one for final answering) to answer each question.\nOne of the reasons we found behind this slow progress in performance is the lack of diversity in\n8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2220, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29238968-d3fb-4d06-9134-8afa9e4f964d": {"__data__": {"id_": "29238968-d3fb-4d06-9134-8afa9e4f964d", "embedding": null, "metadata": {"page_label": "9", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f65b833-e20b-44a0-ab14-0f5e8432248c", "node_type": "4", "metadata": {"page_label": "9", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "hash": "bd6e96a6721e9e1ddf9515a1df3f2fc954aaaffc952214cae3293f65bc3dd6af", "class_name": "RelatedNodeInfo"}}, "text": "Arxiv Preprint.\nPost processingT abular reasoning\nNumerical reasoningMultiple constraints T emporal reasoning\nReasoning Type0.00.10.20.30.40.50.60.7AccuracyAccuracy of Each Reasoning Type\nMulti-step Search (n=5, k=5, n_docs = 10) Multi-step Search Planning (n=5, k=5, n_docs = 10) Oracle\nFigure 5: This plot shows the accuracy of Gemini-Pro-1.5-0514 on each reasoning type in our test set.\nWe observe a significant increase in performance for all reasoning types when we use multi-step search planning,\nwith the performance on numerical reasoning even exceeding oracle performance.\nthe queries generated by the model; it seemed like the model goes in the wrong direction in search\nretrievals and never corrects itself. To mitigate this problem, we experiment with two changes to\nthe instructions: (1) We provide a few examples of how an ideal best-case search query sequence\nshould look, and (2) We provide instructions not to repeat queries and force the model to \"think\nstep-by-step\" (Kojima et al., 2022). We observe a very promising trend with these changes, where\nthe model performance (0.66) through iterations reaches close to the oracle performance (0.73) by\nthe end of five iterations of retrievals. We hope our benchmark will be useful for the community to\nfurther reduce the number of search calls and improve model accuracy.\n4 R ELATED WORKS\nEvaluating Retrieval-Augmented Generation (RAG) systems has become increasingly important as\nthese models integrate retrieval mechanisms with generative capabilities to enhance factual accuracy\nand reasoning(Yu et al., 2024b). Existing benchmarks, such as NaturalQuestions (Kwiatkowski\net al., 2019), TriviaQA (Joshi et al., 2017), and ELI5 (Fan et al., 2019), have been used to\nevaluate RAG models, but they often focus on specific aspects like retrieval accuracy or single-\nturn question answering without considering the full complexity of real-world applications. For\ninstance, NaturalQuestions primarily tests retrieval precision, while TriviaQA emphasizes factual\ncorrectness in trivia-style questions. ELI5, on the other hand, is designed for explainability but does\nnot rigorously assess the multi-hop reasoning necessary for synthesizing information from multiple\nsources. These benchmarks, while valuable, tend to evaluate RAG systems in a piecemeal fashion,\nmissing the comprehensive assessment needed to truly measure their end-to-end capabilities. We\nprovide additional comparisons against other datasets in Table 1.\nFRAMES addresses these limitations by offering a unified and more holistic evaluation framework\nfor RAG systems. Unlike existing datasets, FRAMES tests models across three critical dimensions:\nfactual retrieval, reasoning, and synthesis. It incorporates complex multi-hop queries that require\n9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2778, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c190a6e-d93c-4f8b-8f84-04bf7ee18d64": {"__data__": {"id_": "2c190a6e-d93c-4f8b-8f84-04bf7ee18d64", "embedding": null, "metadata": {"page_label": "10", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a64b368d-09b5-400a-b789-3da3d5174589", "node_type": "4", "metadata": {"page_label": "10", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "hash": "89ddfee4c6a3fae7d3d1b569dddbf34ed8c49b614319e5c05b2d4d8fce443460", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "403c269e-68c6-4600-b043-9e603d85a890", "node_type": "1", "metadata": {}, "hash": "ef1ae2569be56260ffaa8c9b3ecc42c3dce4e10619db6708e104a2ced975829e", "class_name": "RelatedNodeInfo"}}, "text": "Arxiv Preprint.\nmodels to retrieve and integrate information from various sources while also handling temporal\ndisambiguation\u2014a challenge not adequately covered by benchmarks like NaturalQuestions or ELI5.\nAdditionally, FRAMES includes tasks that assess the synthesis of information into coherent and\ncontextually accurate responses, ensuring that RAG systems are evaluated on their ability to perform\nin realistic, multifaceted scenarios. This makes FRAMES a more rigorous and comprehensive\nbenchmark, well suited for guiding the development of next-generation RAG systems.\n5 C ONCLUSION\nIn this work, we introduced FRAMES , a comprehensive evaluation dataset designed to test the\ncapabilities of Retrieval-Augmented Generation (RAG) systems across factuality, retrieval accuracy,\nand reasoning. Our experiments with state-of-the-art LLMs highlight the existing gaps in their ability\nto handle complex, multi-hop reasoning tasks. The baseline results showed that even advanced models\nstruggle significantly with the challenging scenarios presented in FRAMES , achieving only moderate\nimprovements when multi-step retrieval and reasoning strategies were employed. The FRAMES\ndataset addresses a critical need in the evaluation of RAG systems by offering an integrated framework\nthat tests these systems in a more holistic manner compared to existing benchmarks. By simulating\nrealistic, multi-document queries, FRAMES provides a clearer picture of the current capabilities and\nlimitations of LLMs in real-world applications. Our findings underscore the importance of further\nenhancing both the retrieval mechanisms and the reasoning capabilities of these models to improve\ntheir overall performance.\nFuture Work. Moving forward, there are several promising avenues for future research. First, the\ndevelopment of more sophisticated retrieval strategies is essential. This includes exploring dense\nretrievers trained directly on the multihop retrieval task, such as those based on ColBERT (Khattab &\nZaharia, 2020), or SimCSE (Gao et al., 2021) architectures. These approaches could better handle\ndiverse and complex queries by adapting to the context iteratively. Second, improving the reasoning\ncapabilities of LLMs remains a significant challenge. We can explore process supervision methods\nlike those used in PRM-800K (Lightman et al., 2023), or investigate distillation techniques on\nsuccessful trajectories, similar to approaches in ToolFormer (Schick et al., 2024) and DSPy(Khattab\net al., 2023). These methods could enhance numerical, temporal, and post-processing reasoning.\nAdditionally, we can explore modeling approaches such as context reduction of wiki articles to\nimprove planning capabilities and training query generators for more effective information retrieval.\nLastly, expanding the FRAMES dataset to include more diverse and domain-specific questions, as\nwell as incorporating more dynamic elements such as real-time information retrieval, could further\nenhance its utility as a benchmark for next-generation RAG systems. It is important to note that\nfuture work should also address the potential limitations of our current approach, including the risk\nof pretraining data contamination, which may affect the generalizability and reliability of the results,\nparticularly when using Wikipedia articles that could overlap with LLM training data.\nLimitations. While FRAMES provides a comprehensive evaluation framework for RAG systems, it\nis important to acknowledge certain limitations. One significant concern is the potential for pretraining\ndata contamination. As large language models are trained on vast amounts of internet data, there\nis a risk that some of the information in our dataset may have been seen by these models during\ntheir pretraining phase.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3778, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "403c269e-68c6-4600-b043-9e603d85a890": {"__data__": {"id_": "403c269e-68c6-4600-b043-9e603d85a890", "embedding": null, "metadata": {"page_label": "10", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a64b368d-09b5-400a-b789-3da3d5174589", "node_type": "4", "metadata": {"page_label": "10", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "hash": "89ddfee4c6a3fae7d3d1b569dddbf34ed8c49b614319e5c05b2d4d8fce443460", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c190a6e-d93c-4f8b-8f84-04bf7ee18d64", "node_type": "1", "metadata": {"page_label": "10", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "hash": "b143b2a36e170e6d132d4612f6c32cba757ab9380324760952d173c3589232a8", "class_name": "RelatedNodeInfo"}}, "text": "This could lead to artificially inflated performance metrics and reduce the\ndataset\u2019s effectiveness in measuring true generalization capabilities. Future iterations of FRAMES\nshould explore techniques to mitigate this issue, such as using more recent or synthetic data, or\ndeveloping methods to quantify and account for potential contamination. Additionally, while we have\nstrived for diversity in our dataset, it may not fully represent the entire spectrum of real-world queries\nand scenarios, potentially limiting its applicability to certain domains or use cases. Addressing these\nlimitations will be crucial for improving the robustness and reliability of RAG system evaluations.\nREFERENCES\nWenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William Wang. Hybridqa:\nA dataset of multi-hop question answering over tabular and textual data. arXiv preprint\narXiv:2004.07347 , 2020.\n10", "mimetype": "text/plain", "start_char_idx": 3779, "end_char_idx": 4678, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97d0512d-9ce9-452f-a0ca-4b9b81c81602": {"__data__": {"id_": "97d0512d-9ce9-452f-a0ca-4b9b81c81602", "embedding": null, "metadata": {"page_label": "11", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aa8c4472-898a-4470-8577-2738ad4dabc4", "node_type": "4", "metadata": {"page_label": "11", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "hash": "bfc1160bddcc4c183c333ba91ecd0f95e3b04a9df37b913b9765091feacce566", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b07ad015-a3bc-4ed6-b3e8-2321877cfc74", "node_type": "1", "metadata": {}, "hash": "bcb4bad39034cd3220c11f82b6dc3e42c87ce7c48407d63ec4eb35f945022873", "class_name": "RelatedNodeInfo"}}, "text": "Arxiv Preprint.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 ,\n2021.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. Eli5:\nLong form question answering. arXiv preprint arXiv:1907.09190 , 2019.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence\nembeddings. arXiv preprint arXiv:2104.08821 , 2021.\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and\nHaofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv\npreprint arXiv:2312.10997 , 2023.\nTeam Gemma, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya\nBhupatiraju, L\u00e9onard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram\u00e9, et al.\nGemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118 ,\n2024.\nGoogle. Gemini 1.5 flash. https://deepmind.google/technologies/gemini/\nflash/ , 2024a.\nGoogle. Gemini 1.5 pro. https://blog.google/technology/ai/\ngoogle-gemini-next-generation-model-february-2024/ , 2024b.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented\nlanguage model pre-training. In International conference on machine learning , pp. 3929\u20133938.\nPMLR, 2020.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551 , 2017.\nJungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir Radev, Noah A\nSmith, Yejin Choi, Kentaro Inui, et al. Realtime qa: what\u2019s the answer right now? Advances in\nNeural Information Processing Systems , 36, 2024.\nOmar Khattab and Matei Zaharia. Colbert: Efficient and effective passage search via contextualized\nlate interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on\nresearch and development in Information Retrieval , pp. 39\u201348, 2020.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2186, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b07ad015-a3bc-4ed6-b3e8-2321877cfc74": {"__data__": {"id_": "b07ad015-a3bc-4ed6-b3e8-2321877cfc74", "embedding": null, "metadata": {"page_label": "11", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aa8c4472-898a-4470-8577-2738ad4dabc4", "node_type": "4", "metadata": {"page_label": "11", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "hash": "bfc1160bddcc4c183c333ba91ecd0f95e3b04a9df37b913b9765091feacce566", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97d0512d-9ce9-452f-a0ca-4b9b81c81602", "node_type": "1", "metadata": {"page_label": "11", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "hash": "8fb880e47bb76132701d074fe2614366ae62e5e280bbe58ed80adbe4d855aa82", "class_name": "RelatedNodeInfo"}}, "text": "39\u201348, 2020.\nOmar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri\nVardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller,\nMatei Zaharia, and Christopher Potts. Dspy: Compiling declarative language model calls into\nself-improving pipelines. arXiv preprint arXiv:2310.03714 , 2023.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners. Advances in neural information processing systems , 35:\n22199\u201322213, 2022.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a\nbenchmark for question answering research. Transactions of the Association for Computational\nLinguistics , 7:453\u2013466, 2019.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented\ngeneration for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems ,\n33:9459\u20139474, 2020.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step. arXiv preprint\narXiv:2305.20050 , 2023.\n11", "mimetype": "text/plain", "start_char_idx": 2174, "end_char_idx": 3552, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66920057-c461-4dd5-aeea-482ac83ba272": {"__data__": {"id_": "66920057-c461-4dd5-aeea-482ac83ba272", "embedding": null, "metadata": {"page_label": "12", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c2eefe78-a0bc-4384-8cfd-0966a628d74c", "node_type": "4", "metadata": {"page_label": "12", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "hash": "0c9a2c2f319b4cd300dcef94467383eabc14a8f4a2ca7808f4650ab2dff40257", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ed0e6d9-d453-4835-ad1f-672a95ce0685", "node_type": "1", "metadata": {}, "hash": "fee67a44b44de4a88308eb10457511db5a0ee002faa13b4de36b4673fc553c13", "class_name": "RelatedNodeInfo"}}, "text": "Arxiv Preprint.\nStephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human\nfalsehoods. arXiv preprint arXiv:2109.07958 , 2021.\nStephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human\nfalsehoods. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of\nthe 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\nACL 2022, Dublin, Ireland, May 22-27, 2022 , pp. 3214\u20133252. Association for Computational\nLinguistics, 2022. doi: 10.18653/V1/2022.ACL-LONG.229. URL https://doi.org/10.\n18653/v1/2022.acl-long.229 .\nTie-Yan Liu et al. Learning to rank for information retrieval. Foundations and Trends \u00aein Information\nRetrieval , 3(3):225\u2013331, 2009.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\nelectricity? a new dataset for open book question answering. In EMNLP , 2018.\nStephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford,\net al. Okapi at trec-3. Nist Special Publication Sp , 109:109, 1995.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke\nZettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach\nthemselves to use tools. Advances in Neural Information Processing Systems , 36, 2024.\nJulian Schnitzler, Xanh Ho, Jiahao Huang, Florian Boudin, Saku Sugawara, and Akiko Aizawa.\nMorehopqa: More than multi-hop reasoning. arXiv preprint arXiv:2406.13397 , 2024.\nYixuan Tang and Yi Yang. Multihop-rag: Benchmarking retrieval-augmented generation for multi-hop\nqueries. arXiv preprint arXiv:2401.15391 , 2024.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop\nquestions via single-hop question composition. Trans. Assoc. Comput. Linguistics , 10:539\u2013\n554, 2022. doi: 10.1162/TACL\\_A\\_00475. URL https://doi.org/10.1162/tacl_\na_00475 .\nTu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung,\nDenny Zhou, Quoc Le, et al. Freshllms: Refreshing large language models with search engine\naugmentation. arXiv preprint arXiv:2310.03214 , 2023.\nJohannes Welbl, Pontus Stenetorp, and Sebastian Riedel.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2261, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ed0e6d9-d453-4835-ad1f-672a95ce0685": {"__data__": {"id_": "0ed0e6d9-d453-4835-ad1f-672a95ce0685", "embedding": null, "metadata": {"page_label": "12", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c2eefe78-a0bc-4384-8cfd-0966a628d74c", "node_type": "4", "metadata": {"page_label": "12", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "hash": "0c9a2c2f319b4cd300dcef94467383eabc14a8f4a2ca7808f4650ab2dff40257", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "66920057-c461-4dd5-aeea-482ac83ba272", "node_type": "1", "metadata": {"page_label": "12", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "hash": "73cd26f2ca3d33ad3c0c1acf3830756516057ae07ef8a0d6d1c0deb2c65d253a", "class_name": "RelatedNodeInfo"}}, "text": "Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. Constructing datasets for multi-hop reading\ncomprehension across documents. CoRR , abs/1710.06481, 2017. URL http://arxiv.org/\nabs/1710.06481 .\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov,\nand Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question\nanswering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun\u2019ichi Tsujii (eds.),\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,\nBrussels, Belgium, October 31 - November 4, 2018 , pp. 2369\u20132380. Association for Computational\nLinguistics, 2018a. doi: 10.18653/V1/D18-1259. URL https://doi.org/10.18653/v1/\nd18-1259 .\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov,\nand Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question\nanswering. arXiv preprint arXiv:1809.09600 , 2018b.\nHao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu. Evaluation of\nretrieval-augmented generation: A survey. ArXiv , abs/2405.07437, 2024a. URL https:\n//api.semanticscholar.org/CorpusID:269758033 .\nHao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu. Evaluation of retrieval-\naugmented generation: A survey. arXiv preprint arXiv:2405.07437 , 2024b.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,\nBeichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv\npreprint arXiv:2303.18223 , 2023.\n12", "mimetype": "text/plain", "start_char_idx": 2206, "end_char_idx": 3785, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "034c1dce-5fc9-4c86-81bc-e204afa3fbce": {"__data__": {"id_": "034c1dce-5fc9-4c86-81bc-e204afa3fbce", "embedding": null, "metadata": {"page_label": "13", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8f4ba1b4-842a-4a08-951f-8116b6d96196", "node_type": "4", "metadata": {"page_label": "13", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "hash": "006031673559ae157e058a323e399212d54dc18cb9dd681978535e8cad28ff79", "class_name": "RelatedNodeInfo"}}, "text": "Arxiv Preprint.\nA S YNTHETIC DATA GENERATION PROMPT\nSynthetic Data Generation\nSystem: You are a helpful assistant.\nUser: \"\"\"TASK: You will be provided with {k_context} Wikipedia article extracts.\nBased on these extracts, generate {n_questions} challenging factoid questions that meet\nthe following criteria:\n1.Standalone & Context-Independent: Questions should not contain any references to\n\"Article 1\", \"Article 2\", etc. They should be understandable without any additional context.\n2.Unambiguous Answer: Each question should have a single, clear, and factual answer.\n3.Multi-hop Reasoning: Answering each question should require combining information\nfrom ALL {k_context} provided Wikipedia articles.\n4.Grounded in Context & Conceptual Format: Each question must conceptually follow\nthis format, seamlessly integrating information from each article:\n**Start with a clear question word (What/How/Where/When).**\n**Introduce information from each article step-by-step, using connectors to link them\nlogically.**\n**Example connectors: \u2019in relation to\u2019, \u2019compared to\u2019, \u2019as a result of\u2019, \u2019which also\u2019, \u2019in\naddition to\u2019.\n** For each question: *\n**Provide the single-word answer in parentheses after the question mark.** *\n**On a new line, clearly explain the reasoning process.** *\n**For each article, bullet point the specific piece of information used to formulate the\nquestion.**\nExample:\n**Question:** What type of bird, belonging to the Ardeidae family, went extinct around\n1690 and was known for its terrestrial abilities? (Dodo)\n**Reasoning:** * **Article 1:** Provides information about the Dodo belonging to the\nArdeidae family. * **Article 2:** Mentions the extinction of the Dodo around 1690. *\n**Article 3:** Highlights the Dodo\u2019s adaptation to terrestrial life.\n{WIKI ARTICLES} \"\"\"\nFigure 6: Prompt used to generate questions synthetically using Gemini-Pro-1.5-0514 .k_context and\nn_questions are placeholders for the number of articles provided and the number of questions to generate\nper inference.\nB A UTORATER PROMPT\n13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2031, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd9731a7-724c-4f69-94f0-abe5d250c377": {"__data__": {"id_": "bd9731a7-724c-4f69-94f0-abe5d250c377", "embedding": null, "metadata": {"page_label": "14", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d27039c7-78a5-4e9f-a358-dfffd25441c8", "node_type": "4", "metadata": {"page_label": "14", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}, "hash": "1eb98eb3213ce0861307277ea3c124481696d19659fac2c7c9b09a2f600fdd5a", "class_name": "RelatedNodeInfo"}}, "text": "Arxiv Preprint.\nAuto-rating Prompt\nSystem: You are a helpful assistant.\nUser: \"\"\"===Task===\nI need your help in evaluating an answer provided by an LLM against a ground\ntruth answer. Your task is to determine if the ground truth answer is present in the LLM\u2019s\nresponse. Please analyze the provided data and make a decision.\n===Instructions===\n1. Carefully compare the \"Predicted Answer\" with the \"Ground Truth Answer\".\n2. Consider the substance of the answers \u2013 look for equivalent information or correct answers.\nDo not focus on exact wording unless the exact wording is crucial to the meaning.\n3. Your final decision should be based on whether the meaning and the vital facts of the\n\"Ground Truth Answer\" are present in the \"Predicted Answer:\"\n===Input Data===\n- Question: \u00abquestion\u00bb\n- Predicted Answer: \u00abLLM_response\u00bb\n- Ground Truth Answer: \u00abground_truth_answer\u00bb\n===Output Format===\nProvide your final evaluation in the following format:\n\"Explanation:\" (How you made the decision?)\n\"Decision:\" (\"TRUE\" or \"FALSE\" )\nPlease proceed with the evaluation.\"\"\" \"\"\"\nFigure 7: Prompt used to auto-rate the responses of LLM in the experiments. The LLM is provided with\nquestions, model responses, and ground truth answers, along with instructions to check if the model response\ncontains the gold answer.\n14", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1299, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"1d99d66a-a2c2-42ca-af4d-081762592ca5": {"node_ids": ["a610bb39-38b6-4977-b388-266a83ac9301", "99f341ba-5b9e-4acb-90a1-94ea8365ae7e"], "metadata": {"page_label": "1", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}}, "24bad57e-9733-4746-93fc-43dda2c3651c": {"node_ids": ["8a160006-3b02-45bf-8bf3-e4ca5ad20152", "22dbc33e-29bd-4b6b-8441-f1f3bc4f59a0"], "metadata": {"page_label": "2", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}}, "2e1fb198-b9e2-421f-94dd-0ec449367447": {"node_ids": ["41023438-3c98-4ede-9bfc-2fe8ca5bea76"], "metadata": {"page_label": "3", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}}, "5563b9e3-1b4e-4016-b898-e2ae768e8ffc": {"node_ids": ["e8f5a8bb-2464-46a0-b4eb-01eb6cf52389", "f07bbb01-f6b9-44e8-ace8-ade73b2273b6"], "metadata": {"page_label": "4", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}}, "cb94d43e-267f-4d9d-9fff-fde45c82e081": {"node_ids": ["039607ce-6ad3-4464-b98c-86ae000c26ed"], "metadata": {"page_label": "5", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}}, "e699b846-2996-42b5-9b4c-bd23f86b09a5": {"node_ids": ["d95d3dca-57b1-4063-a278-1b95e01c0632", "73ce5d80-55df-4d82-8396-8ada1c70f9a1"], "metadata": {"page_label": "6", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}}, "a1002e05-5a41-4553-bd02-eff4b776a108": {"node_ids": ["cf26f1c5-5241-4cf8-bdf4-2d1421f04e25"], "metadata": {"page_label": "7", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}}, "7e4ea3ec-e9c3-48b4-bfc7-5e47c02a4862": {"node_ids": ["cad9dfd7-db87-4e14-bb98-efeb9db4fdf8"], "metadata": {"page_label": "8", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}}, "2f65b833-e20b-44a0-ab14-0f5e8432248c": {"node_ids": ["29238968-d3fb-4d06-9134-8afa9e4f964d"], "metadata": {"page_label": "9", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}}, "a64b368d-09b5-400a-b789-3da3d5174589": {"node_ids": ["2c190a6e-d93c-4f8b-8f84-04bf7ee18d64", "403c269e-68c6-4600-b043-9e603d85a890"], "metadata": {"page_label": "10", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}}, "aa8c4472-898a-4470-8577-2738ad4dabc4": {"node_ids": ["97d0512d-9ce9-452f-a0ca-4b9b81c81602", "b07ad015-a3bc-4ed6-b3e8-2321877cfc74"], "metadata": {"page_label": "11", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}}, "c2eefe78-a0bc-4384-8cfd-0966a628d74c": {"node_ids": ["66920057-c461-4dd5-aeea-482ac83ba272", "0ed0e6d9-d453-4835-ad1f-672a95ce0685"], "metadata": {"page_label": "12", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}}, "8f4ba1b4-842a-4a08-951f-8116b6d96196": {"node_ids": ["034c1dce-5fc9-4c86-81bc-e204afa3fbce"], "metadata": {"page_label": "13", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}}, "d27039c7-78a5-4e9f-a358-dfffd25441c8": {"node_ids": ["bd9731a7-724c-4f69-94f0-abe5d250c377"], "metadata": {"page_label": "14", "file_name": "rag.pdf", "file_path": "/tmp/8a48817a-f6b1-4290-af0f-56ac3dec831b/rag.pdf", "file_type": "application/pdf", "file_size": 902232, "creation_date": "2024-10-21", "last_modified_date": "2024-10-21"}}}}